<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>DexDiff</title>
    <!-- Bulma CSS  -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <!-- Font Awesome icons  -->
    <script src="https://kit.fontawesome.com/b165665cab.js" crossorigin="anonymous"></script>
</head>

<body>
    <div class="container p-3 is-primary">
        <!-- Title section -->
        <div class="block is-center has-text-centered">
            <h1 class="title is-1 is-spaced">DexDiff</h1>
            <div style="font-size:25px;" class="columns is-mobile is-centered is-multiline"> xxx
            </div>
            <div class="columns is-mobile is-centered is-multiline"> 
            </div>
        </div>

        <div>
            <p style="text-align:center;"><img src="./framework2.png" alt="intro"style="width:70%;" /> </p>
        </div>


        <!-- Abstract -->
        <div class="block box" class="has-text-centered">
            <div class="has-text-centered">
                <h2 class="title is-3"> Abstract </h2>
                <p class="is-size-5">
                    Grasping large and flat objects (e.g. a book or a pan) is often regarded as an \textit{ungraspable} task, which poses significant challenges due to the unreachable grasping poses. Previous works leverage \textit{Extrinsic Dexterity} like walls or table edges to grasp such objects. However, they are limited to task-specific policies and lack task planning to find pre-grasp conditions. This makes it difficult to adapt to various environments and extrinsic dexterity constraints. Therefore, we present DexDiff, a robust robotic manipulation method for long-horizon planning with extrinsic dexterity. Specifically, we utilize a vision-language model (VLM) to perceive the environmental state and generate high-level task plans, followed by a goal-conditioned action diffusion (GCAD) model to predict the sequence of low-level actions. This model learns the low-level policy from offline data with the cumulative reward guided by high-level planning as the goal condition, which allows for improved prediction of robot actions. Experimental results demonstrate that our method not only effectively performs ungraspable tasks but also generalizes to previously unseen objects. It outperforms baselines by a 47\% higher success rate in simulation and facilitates efficient deployment and manipulation in real-world scenarios.
                </p>
            </div>
        </div>


    </div>
    <footer class="footer pt-5 pb-5">
        <div class="content has-text-centered">
            <p>
                Page source code borrowed from 
                <a href="https://mazpie.github.io"><strong>Pietro Mazzaglia</strong></a>.
                <br>
            </p>
        </div>
    </footer>
</body>

</html>
